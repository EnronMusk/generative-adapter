{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import collections\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_all_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(data, save_path):\n",
    "    import json, os\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    with open(save_path, 'w') as f:\n",
    "        for item in data:\n",
    "            id = item['id']\n",
    "            context = item['context']\n",
    "            conversations = []\n",
    "            for qa in item['qa_pairs']:\n",
    "                question = qa['question']\n",
    "                answer = qa['answer']\n",
    "                conversations.append({\"role\": \"user\", \"content\": question})\n",
    "                conversations.append({\"role\": \"assistant\", \"content\": answer})\n",
    "            f.write(json.dumps({\"id\": id, \"context\": context, \"conversations\": conversations}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COQA ('stanfordnlp/coqa')\n",
    "output_list = []\n",
    "data = datasets.load_dataset('stanfordnlp/coqa')\n",
    "data = list(data['train'])\n",
    "# iterate all items in the dataset\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "item_list = []\n",
    "for i, item in enumerate(data):\n",
    "    context = item['story']\n",
    "    questions = item['questions']\n",
    "    answers = item['answers'][\"input_text\"]\n",
    "    item_list.append({\n",
    "        'context': context,\n",
    "        'qa_pairs': [{'question': q, 'answer': a} for q, a in zip(questions, answers)]\n",
    "    })\n",
    "\n",
    "# randomly merge five of each to one item\n",
    "num_context = 8\n",
    "num_merge = 1\n",
    "for i in range(0, len(item_list), num_merge):\n",
    "    if i + num_merge > len(item_list):\n",
    "        break\n",
    "    context_list = [item_list[j]['context'] for j in range(i, i + num_merge)]\n",
    "    context_list += [item['context'] for item in random.sample(item_list, random.randint(0, num_context))]\n",
    "    random.shuffle(context_list)\n",
    "    context = \"\\n\\n\".join(context_list)\n",
    "    if len(context.split()) > 4096:\n",
    "        print(f\"skip context len: {len(context.split())}\")\n",
    "        continue\n",
    "    qa_pairs_list = [item_list[j]['qa_pairs'] for j in range(i, i + num_merge)]\n",
    "    random.shuffle(qa_pairs_list)\n",
    "    qa_pairs = sum(qa_pairs_list, [])\n",
    "    while sum([len(qa[\"question\"].split()) for qa in qa_pairs]) + sum([len(qa[\"answer\"].split()) for qa in qa_pairs]) > 512:\n",
    "        qa_pairs = qa_pairs[:-1]\n",
    "        # print(f\"q eln: {[len(qa['question'].split()) for qa in qa_pairs]}, a len: {[len(qa['answer'].split()) for qa in qa_pairs]}\")\n",
    "        # continue\n",
    "    output_list.append({\n",
    "        \"id\": f\"coqa.{len(output_list):04d}\",\n",
    "        'context': context,\n",
    "        'qa_pairs': qa_pairs\n",
    "    })\n",
    "\n",
    "print(f\"COQA: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "print(collections.Counter([len(item[\"qa_pairs\"]) for item in output_list]))\n",
    "save(output_list, 'v4/coqa.jsonl')\n",
    "# data_all_list.extend(output_list)\n",
    "# # output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP ('ucinlp/drop')\n",
    "output_list = []\n",
    "data = datasets.load_dataset('ucinlp/drop')\n",
    "data = list(data['train'])\n",
    "random.seed(0)\n",
    "\n",
    "context_to_item = {}\n",
    "for item in data:\n",
    "    context = item['passage']\n",
    "    if context not in context_to_item:\n",
    "        context_to_item[context] = []\n",
    "    context_to_item[context].append(item)\n",
    "\n",
    "\n",
    "item_list = []\n",
    "for i, context in enumerate(context_to_item):\n",
    "    qa_pairs = []\n",
    "    for item in context_to_item[context]:\n",
    "        question = item['question']\n",
    "        answer = item['answers_spans']['spans'][0]\n",
    "        qa_pairs.append({'question': question, 'answer': answer})\n",
    "    item_list.append({\n",
    "        'context': context,\n",
    "        'qa_pairs': qa_pairs,\n",
    "    })\n",
    "\n",
    "\n",
    "num_context = 8\n",
    "num_merge = 1\n",
    "for i in range(0, len(item_list), num_merge):\n",
    "    if i + num_merge > len(item_list):\n",
    "        break\n",
    "    context_list = [item_list[j]['context'] for j in range(i, i + num_merge)]\n",
    "    context_list += [item['context'] for item in random.sample(item_list, random.randint(0, num_context))]\n",
    "    random.shuffle(context_list)\n",
    "    context = \"\\n\\n\".join(context_list)\n",
    "    if len(context.split()) > 4096:\n",
    "        print(f\"skip context len: {len(context.split())}\")\n",
    "        continue\n",
    "    qa_pairs_list = [item_list[j]['qa_pairs'] for j in range(i, i + num_merge)]\n",
    "    random.shuffle(qa_pairs_list)\n",
    "    qa_pairs = sum(qa_pairs_list, [])\n",
    "    while sum([len(qa[\"question\"].split()) for qa in qa_pairs]) + sum([len(qa[\"answer\"].split()) for qa in qa_pairs]) > 512:\n",
    "        qa_pairs = qa_pairs[:-1]\n",
    "        # print(f\"q eln: {[len(qa['question'].split()) for qa in qa_pairs]}, a len: {[len(qa['answer'].split()) for qa in qa_pairs]}\")\n",
    "        # continue\n",
    "    output_list.append({\n",
    "        \"id\": f\"drop.{len(output_list):04d}\",\n",
    "        'context': context,\n",
    "        'qa_pairs': qa_pairs\n",
    "    })\n",
    "\n",
    "print(f\"DROP: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "print(collections.Counter([len(item[\"qa_pairs\"]) for item in output_list]))\n",
    "save(output_list, 'v4/drop.jsonl')\n",
    "# data_all_list.extend(output_list)\n",
    "# output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# narrativeqa ('deepmind/narrativeqa')\n",
    "output_list = []\n",
    "data = datasets.load_dataset('deepmind/narrativeqa')\n",
    "data = list(data['train'])\n",
    "context_to_item = {}\n",
    "for item in data:\n",
    "    context = item['document']['summary']['text']\n",
    "    if context not in context_to_item:\n",
    "        context_to_item[context] = []\n",
    "    context_to_item[context].append(item)\n",
    "for i, context in enumerate(context_to_item):\n",
    "    if len(context.split()) > 2048:\n",
    "        print(f\"skip context len: {len(context.split())}\")\n",
    "        continue\n",
    "    qa_pairs = []\n",
    "    for item in context_to_item[context]:\n",
    "        question = item['question']['text']\n",
    "        answer = item['answers'][0]['text']\n",
    "        qa_pairs.append({'question': question, 'answer': answer})\n",
    "    while sum([len(qa[\"question\"].split()) for qa in qa_pairs]) + sum([len(qa[\"answer\"].split()) for qa in qa_pairs]) > 512:\n",
    "        qa_pairs = qa_pairs[:-1]\n",
    "        # print(f\"q eln: {[len(qa['question'].split()) for qa in qa_pairs]}, a len: {[len(qa['answer'].split()) for qa in qa_pairs]}\")\n",
    "        # continue\n",
    "    output_list.append({\n",
    "        \"id\": f\"narrativeqa.{i:04d}\",\n",
    "        'context': context,\n",
    "        'qa_pairs': qa_pairs\n",
    "    })\n",
    "print(f\"narrativeqa: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "print(collections.Counter([len(item[\"qa_pairs\"]) for item in output_list]))\n",
    "save(output_list, 'v4/narrativeqa.jsonl')\n",
    "# output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PubMed ('qiaojin/PubMedQA')\n",
    "output_list = []\n",
    "data = datasets.load_dataset('qiaojin/PubMedQA', 'pqa_labeled')\n",
    "data = list(data['train'])\n",
    "context_to_item = {}\n",
    "for item in data:\n",
    "    context = \"\\n\".join(item['context']['contexts'])\n",
    "    if context not in context_to_item:\n",
    "        context_to_item[context] = []\n",
    "    context_to_item[context].append(item)\n",
    "for i, context in enumerate(context_to_item):\n",
    "    if len(context.split()) > 2048:\n",
    "        print(f\"skip context len: {len(context.split())}\")\n",
    "        continue\n",
    "    qa_pairs = []\n",
    "    assert len(context_to_item[context]) == 1\n",
    "    for item in context_to_item[context]:\n",
    "        question = item['question']\n",
    "        answer = f'{item[\"final_decision\"].capitalize()}. {item[\"long_answer\"]}'\n",
    "        qa_pairs.append({'question': question, 'answer': answer})\n",
    "    if sum([len(qa[\"question\"].split()) for qa in qa_pairs]) + sum([len(qa[\"answer\"].split()) for qa in qa_pairs]) > 512:\n",
    "        print(f\"q eln: {[len(qa['question'].split()) for qa in qa_pairs]}, a len: {[len(qa['answer'].split()) for qa in qa_pairs]}\")\n",
    "        continue\n",
    "    output_list.append({\n",
    "        \"id\": f\"pubmedqa.{i:04d}\",\n",
    "        'context': context,\n",
    "        'qa_pairs': qa_pairs\n",
    "    })\n",
    "print(f\"pubmedqa: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "print(collections.Counter([len(item[\"qa_pairs\"]) for item in output_list]))\n",
    "save(output_list, 'v4/pubmedqa.jsonl')\n",
    "# data_all_list.extend(output_list)\n",
    "# output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quail ('textmachinelab/quail')\n",
    "output_list = []\n",
    "data = datasets.load_dataset('textmachinelab/quail')\n",
    "data = list(data['train'])\n",
    "context_to_item = {}\n",
    "for item in data:\n",
    "    context = item['context']\n",
    "    if context not in context_to_item:\n",
    "        context_to_item[context] = []\n",
    "    context_to_item[context].append(item)\n",
    "for i, context in enumerate(context_to_item):\n",
    "    if len(context.split()) > 2048:\n",
    "        print(f\"skip context len: {len(context.split())}\")\n",
    "        continue\n",
    "    qa_pairs = []\n",
    "    for item in context_to_item[context]:\n",
    "        question = item['question']\n",
    "        answer = item['answers'][item['correct_answer_id']]\n",
    "        if answer == \"not enough information\":\n",
    "            continue\n",
    "        qa_pairs.append({'question': question, 'answer': answer})\n",
    "    if len(qa_pairs) == 0:\n",
    "        continue\n",
    "    if sum([len(qa[\"question\"].split()) for qa in qa_pairs]) + sum([len(qa[\"answer\"].split()) for qa in qa_pairs]) > 512:\n",
    "        print(f\"q eln: {[len(qa['question'].split()) for qa in qa_pairs]}, a len: {[len(qa['answer'].split()) for qa in qa_pairs]}\")\n",
    "        continue\n",
    "    output_list.append({\n",
    "        \"id\": f\"quail.{i:04d}\",\n",
    "        'context': context,\n",
    "        'qa_pairs': qa_pairs\n",
    "    })\n",
    "print(f\"quail: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "print(collections.Counter([len(item[\"qa_pairs\"]) for item in output_list]))\n",
    "save(output_list, 'v4/quail.jsonl')\n",
    "# output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # quail ('rajpurkar/squad_v2')\n",
    "# output_list = []\n",
    "# data = datasets.load_dataset('rajpurkar/squad_v2')\n",
    "# data = list(data['train'])\n",
    "# context_to_item = {}\n",
    "# for item in data:\n",
    "#     context = item['context']\n",
    "#     if context not in context_to_item:\n",
    "#         context_to_item[context] = []\n",
    "#     context_to_item[context].append(item)\n",
    "# for i, context in enumerate(context_to_item):\n",
    "#     if len(context.split()) > 1024:\n",
    "#         print(f\"skip context len: {len(context.split())}\")\n",
    "#         continue\n",
    "#     qa_pairs = []\n",
    "#     for item in context_to_item[context]:\n",
    "#         if len(item['answers'][\"text\"]) == 0:\n",
    "#             continue\n",
    "#         question = item['question']\n",
    "#         answer = item['answers'][\"text\"][0]\n",
    "#         qa_pairs.append({'question': question, 'answer': answer})\n",
    "#     if len(qa_pairs) == 0:\n",
    "#         continue\n",
    "#     output_list.append({\n",
    "#         \"id\": f\"squad_v2.{i:04d}\",\n",
    "#         'context': context,\n",
    "#         'qa_pairs': qa_pairs\n",
    "#     })\n",
    "# print(f\"squad_v2: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "# print(collections.Counter([len(item[\"qa_pairs\"]) for item in output_list]))\n",
    "# data_all_list.extend(output_list)\n",
    "# # output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ms marco ('microsoft/ms_marco')\n",
    "output_list = []\n",
    "data = datasets.load_dataset('microsoft/ms_marco', 'v1.1')\n",
    "data = list(data['train'])\n",
    "random.seed(0)\n",
    "\n",
    "context_to_item = {}\n",
    "for item in data:\n",
    "    passages = item['passages']\n",
    "    if sum(passages['is_selected']) == 0:\n",
    "        continue\n",
    "    is_selected_idx = passages['is_selected'].index(1)\n",
    "    assert isinstance(is_selected_idx, int)\n",
    "    context = passages['passage_text'][is_selected_idx]\n",
    "    if context not in context_to_item:\n",
    "        context_to_item[context] = []\n",
    "    context_to_item[context].append(item)\n",
    "item_list = []\n",
    "for i, context in enumerate(context_to_item):\n",
    "    qa_pairs = []\n",
    "    for item in context_to_item[context]:\n",
    "        if len(item['answers']) == 0:\n",
    "            continue\n",
    "        question = item['query']\n",
    "        answer = item['answers'][0]\n",
    "        qa_pairs.append({'question': question, 'answer': answer})\n",
    "    if len(qa_pairs) == 0:\n",
    "        continue\n",
    "    item_list.append({\n",
    "        'context': context,\n",
    "        'qa_pairs': qa_pairs,\n",
    "    })\n",
    "# randomly merge five of each to one item\n",
    "num_context = 32\n",
    "num_merge = 4\n",
    "for i in range(0, len(item_list), num_merge):\n",
    "    if i + num_merge > len(item_list):\n",
    "        break\n",
    "    context_list = [item_list[j]['context'] for j in range(i, i + num_merge)]\n",
    "    context_list += [item['context'] for item in random.sample(item_list, random.randint(0, num_context))]\n",
    "    random.shuffle(context_list)\n",
    "    context = \"\\n\\n\".join(context_list)\n",
    "    if len(context.split()) > 4096:\n",
    "        print(f\"skip context len: {len(context.split())}\")\n",
    "        continue\n",
    "    qa_pairs_list = [item_list[j]['qa_pairs'] for j in range(i, i + num_merge)]\n",
    "    random.shuffle(qa_pairs_list)\n",
    "    qa_pairs = sum(qa_pairs_list, [])\n",
    "    while sum([len(qa[\"question\"].split()) for qa in qa_pairs]) + sum([len(qa[\"answer\"].split()) for qa in qa_pairs]) > 512:\n",
    "        qa_pairs = qa_pairs[:-1]\n",
    "        # print(f\"q eln: {[len(qa['question'].split()) for qa in qa_pairs]}, a len: {[len(qa['answer'].split()) for qa in qa_pairs]}\")\n",
    "        # continue\n",
    "    output_list.append({\n",
    "        \"id\": f\"msmarco.{len(output_list):04d}\",\n",
    "        'context': context,\n",
    "        'qa_pairs': qa_pairs\n",
    "    })\n",
    "\n",
    "print(f\"msmarco: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "print(collections.Counter([len(item[\"qa_pairs\"]) for item in output_list]))\n",
    "save(output_list, 'v4/msmarco.jsonl')\n",
    "\n",
    "# data_all_list.extend(output_list)\n",
    "# output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PwC\n",
    "output_list = []\n",
    "data = datasets.load_dataset('sggetao/PwC')\n",
    "data = list(data['train'])\n",
    "context_to_item = {}\n",
    "for item in data:\n",
    "    context = item['input']\n",
    "    if context not in context_to_item:\n",
    "        context_to_item[context] = []\n",
    "    context_to_item[context].append(item)\n",
    "for i, context in enumerate(context_to_item):\n",
    "    if len(context.split()) > 2048:\n",
    "        print(f\"skip context len: {len(context.split())}\")\n",
    "        continue\n",
    "    qa_pairs = []\n",
    "    for item in context_to_item[context]:\n",
    "        if len(item['answer'].split()) > 128 or len(item['prompt'].split()) > 128:\n",
    "            continue\n",
    "        question = item['prompt']\n",
    "        answer = item['answer']\n",
    "        qa_pairs.append({'question': question, 'answer': answer})\n",
    "    # randomly sample 5 qa pairs for each context\n",
    "    # qa_pairs = qa_pairs[:8]\n",
    "    while sum([len(qa[\"question\"].split()) for qa in qa_pairs]) + sum([len(qa[\"answer\"].split()) for qa in qa_pairs]) > 512:\n",
    "        qa_pairs = qa_pairs[:-1]\n",
    "        # print(f\"q eln: {[len(qa['question'].split()) for qa in qa_pairs]}, a len: {[len(qa['answer'].split()) for qa in qa_pairs]}\")\n",
    "        continue\n",
    "    output_list.append({\n",
    "        \"id\": f\"pwc.{i:04d}\",\n",
    "        'context': context,\n",
    "        'qa_pairs': qa_pairs\n",
    "    })\n",
    "print(f\"pwc: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "print(collections.Counter([len(item[\"qa_pairs\"]) for item in output_list]))\n",
    "save(output_list, 'v4/pwc.jsonl')\n",
    "# output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, torch, transformers, random\n",
    "from tqdm import tqdm\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "\n",
    "output_list = []\n",
    "random.seed(0)\n",
    "\n",
    "with open(\"../../data/metaicl/hr_to_lr.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "for task in tqdm(config[\"train\"]):\n",
    "    dp = torch.load(f'../../data/metaicl/llama/{task}/{task}_16384_100_train.jsonl')\n",
    "    k_shot = 64\n",
    "    k_test = 4\n",
    "    max_num = 200\n",
    "    n = len(dp['input'])\n",
    "\n",
    "\n",
    "    input_text = [tokenizer.decode(dp['input'][i]) for i in range(n)]\n",
    "    output_text = [tokenizer.decode(dp['output'][i]) for i in range(n)]\n",
    "    avg_input_len = sum([len(x.split()) for x in input_text]) / n\n",
    "    k_shot = min(k_shot, 2048 // int(avg_input_len))\n",
    "    k_test = min(k_test, 512 // int(avg_input_len))\n",
    "    \n",
    "    if k_shot == 0 or k_test == 0:\n",
    "        print(f\"skip {task} due to avg_input_len: {avg_input_len}\")\n",
    "        continue\n",
    "\n",
    "    # reduce the number of examples\n",
    "    n = min(n, max_num * (k_shot + k_test))\n",
    "    index = random.sample(range(len(input_text)), n)\n",
    "    input_text = [input_text[i] for i in index]\n",
    "    output_text = [output_text[i] for i in index]\n",
    "\n",
    "    # first k are demonstrations, the rest are test case\n",
    "    for i in range(0, n, k_shot + k_test):\n",
    "        if i + k_shot + k_test > n:\n",
    "            break\n",
    "        context = \"\\n\\n\".join([f\"Input: {input_text[j]}\\nOutput: {output_text[j]}\" for j in range(i, i + k_shot)])\n",
    "        # test_input_text = f\"Input: {input_text[i + k]}\\nOutput: \"\n",
    "        # test_output_text = f\"{output_text[i + k]}\"\n",
    "        output_list.append({\n",
    "            \"id\": f\"metaicl.{len(output_list):04d}.{task}.{i // (k_shot + k_test)}\",\n",
    "            'context': context,\n",
    "            'qa_pairs': [\n",
    "                {'question': f\"Input: {input_text[j]}\\nOutput:\", 'answer': output_text[j]} for j in range(i + k_shot, i + k_shot + k_test)\n",
    "            ],\n",
    "        })\n",
    "    print(task, n, avg_input_len, k_shot, k_test, len(output_list))\n",
    "print(f\"metaicl: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "print(collections.Counter([len(item[\"qa_pairs\"]) for item in output_list]))\n",
    "save(output_list, 'v4/metaicl.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Long Alpaca\n",
    "# data_path = \"../../data/long-llm/longalpaca/train.json\"\n",
    "# data = datasets.load_dataset('json', data_files=data_path)\n",
    "data = datasets.load_dataset('Yukang/LongAlpaca-12k')\n",
    "data = list(data[\"train\"])\n",
    "output_list = []\n",
    "for i, item in enumerate(data):\n",
    "    first_message = item[\"instruction\"]\n",
    "    second_message = item[\"output\"]\n",
    "    match_success = False\n",
    "    try:\n",
    "        result = re.search(r\"^(.*)Now the (.*?) ends\\.(.*?)$\", first_message, re.DOTALL)\n",
    "        assert result is not None, f\"id: {i}\"\n",
    "        instruction = result.group(3).strip()\n",
    "        context = first_message[:first_message.index(result.group(3))].strip()\n",
    "        response = second_message\n",
    "        match_success = True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if not match_success:\n",
    "        continue\n",
    "\n",
    "    # if the lenth of the context exeed 8192, skip\n",
    "    if len(context.split()) > 6144:\n",
    "        continue    \n",
    "\n",
    "    output_list.append({\n",
    "        \"id\": f\"longalpaca.{i:05d}\",\n",
    "        \"context\": context,\n",
    "        \"conversations\": [\n",
    "            {\"role\": \"user\", \"content\": instruction},\n",
    "            {\"role\": \"assistant\", \"content\": response}\n",
    "        ]\n",
    "    })\n",
    "# data_all_list.extend(output_list)\n",
    "print(f\"LONGALPACA: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "print(collections.Counter([len(item[\"conversations\"]) for item in output_list]))\n",
    "# save output_list to v2/longalpaca.jsonl\n",
    "with open(\"v4/longalpaca.jsonl\", \"w\") as f:\n",
    "    for item in output_list:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# booksum\n",
    "output_list = []\n",
    "data = datasets.load_dataset('kmfoda/booksum')\n",
    "data = list(data['train'])\n",
    "# iterate all items in the dataset\n",
    "for i, item in enumerate(data):\n",
    "    assert item is not None\n",
    "    book_id = item[\"book_id\"].split(\".\")[0]\n",
    "    summary_id = item[\"summary_id\"]\n",
    "    context = f\"{book_id}, {summary_id}:\\n\\n{item['chapter']}\"\n",
    "    question = f\"Summarize {book_id}, {summary_id}\"\n",
    "    answer = item['summary_text']\n",
    "    if len(context.split()) > 4096:\n",
    "        # print(f\"skip context len: {len(context.split())}\")\n",
    "        continue\n",
    "    if len(question.split()) + len(answer.split()) > 512:\n",
    "        # print(f\"q len: {len(question.split())}, a len: {len(answer.split())}\")\n",
    "        continue\n",
    "    output_list.append({\n",
    "        \"id\": f\"booksum.{i:04d}\",\n",
    "        'context': context,\n",
    "        'qa_pairs': [{'question': question, 'answer': answer}]\n",
    "    })\n",
    "print(f\"booksum: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "print(collections.Counter([len(item[\"qa_pairs\"]) for item in output_list]))\n",
    "save(output_list, 'v4/booksum.jsonl')\n",
    "# data_all_list.extend(output_list)\n",
    "# # output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all \"v4/*.jsonl\" to \"sft-v4.jsonl\"\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "config = {\n",
    "    \"sft-v4-qa.jsonl\": [\"coqa\", \"drop\", \"narrativeqa\", \"pubmedqa\", \"quail\", \"msmarco\"],\n",
    "    \"sft-v4-ift.jsonl\": [\"coqa\", \"drop\", \"narrativeqa\", \"pubmedqa\", \"quail\", \"msmarco\", \"booksum\", \"longalpaca\"],\n",
    "    \"sft-v4.jsonl\": [\"coqa\", \"drop\", \"narrativeqa\", \"pubmedqa\", \"quail\", \"msmarco\", \"booksum\", \"longalpaca\", \"metaicl\"],\n",
    "}\n",
    "\n",
    "for output_path, dataset_names in config.items():\n",
    "    output_list = []\n",
    "    for dataset_name in dataset_names:\n",
    "        with open(f\"v4/{dataset_name}.jsonl\", \"r\") as f:\n",
    "            data = [json.loads(line) for line in f]\n",
    "        output_list.extend(data)\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for item in output_list:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "    print(f\"{output_path}: {len(output_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "with open(\"sft-v4.jsonl\", \"r\") as f:\n",
    "    output_list = [json.loads(line) for line in f]\n",
    "\n",
    "# measure \"#docs\t#instructions\tdoc len\tinstruction len\tresponse len\"\n",
    "stat_list = []\n",
    "for item in output_list:\n",
    "    dataset = item['id'].split(\".\")[0]\n",
    "    n_docs = 1\n",
    "    n_instructions = sum([x['role'] == 'user' for x in item['conversations']])\n",
    "    n_responses = sum([x['role'] == 'assistant' for x in item['conversations']])\n",
    "    doc_len = len(item['context'].split())\n",
    "    instruction_len_mean = sum([len(x['content'].split()) for x in item['conversations'] if x['role'] == 'user']) / n_instructions\n",
    "    response_len_mean = sum([len(x['content'].split()) for x in item['conversations'] if x['role'] == 'assistant']) / n_responses\n",
    "    instruction_len_max = max([len(x['content'].split()) for x in item['conversations'] if x['role'] == 'user'])\n",
    "    response_len_max = max([len(x['content'].split()) for x in item['conversations'] if x['role'] == 'assistant'])\n",
    "    instruction_len_min = min([len(x['content'].split()) for x in item['conversations'] if x['role'] == 'user'])\n",
    "    response_len_min = min([len(x['content'].split()) for x in item['conversations'] if x['role'] == 'assistant'])\n",
    "    instruction_len_sum = sum([len(x['content'].split()) for x in item['conversations'] if x['role'] == 'user'])\n",
    "    response_len_sum = sum([len(x['content'].split()) for x in item['conversations'] if x['role'] == 'assistant'])\n",
    "    stat_list.append({\n",
    "        \"dataset\": dataset,\n",
    "        \"n_docs\": n_docs,\n",
    "        \"n_instructions\": n_instructions,\n",
    "        \"n_responses\": n_responses,\n",
    "        \"doc_len\": doc_len,\n",
    "        \"instruction_len_mean\": instruction_len_mean,\n",
    "        \"response_len_mean\": response_len_mean,\n",
    "        \"instruction_len_max\": instruction_len_max,\n",
    "        \"response_len_max\": response_len_max,\n",
    "        \"instruction_len_min\": instruction_len_min,\n",
    "        \"response_len_min\": response_len_min,\n",
    "        \"instruction_len_sum\": instruction_len_sum,\n",
    "        \"response_len_sum\": response_len_sum,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(stat_list)\n",
    "df.groupby(\"dataset\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby(\"dataset\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby(\"dataset\").max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby(\"dataset\").min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot distribution of df[\"doc_len\"] \n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.histplot(df[\"doc_len\"], bins=100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-adp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
